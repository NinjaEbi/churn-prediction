# -*- coding: utf-8 -*-
"""Customer_Churn_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GHKjagMMNwPbaeMV49OXoUGw5H8RRVes
"""
#!pip install scikit-learn seaborn

"""**Customer Churn Prediction**

Objective: Build a machine learning model to predict customer churn (whether a customer will leave or stay).  
**Industry relevance:** Widely used in Telecom, E-commerce, SaaS.  
**ML Task:** Binary Classification (Churn = Yes/No).

"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

# Upload CSV via left sidebar in Colab before running this
df = pd.read_csv("Telco-Customer-Churn.csv")

# Preview data
df.head()

"""### Dataset Overview
We check column types, missing values, and class distribution.

"""

print(df.info())
print("\nClass Distribution:\n", df["Churn"].value_counts())

# Convert TotalCharges to numeric (fix blank entries)
df["TotalCharges"] = pd.to_numeric(df["TotalCharges"], errors='coerce')
df.dropna(inplace=True)

# Encode target variable
df["Churn"] = df["Churn"].map({"Yes":1, "No":0})

# Encode categorical columns
le = LabelEncoder()
for col in df.select_dtypes(include="object"):
    df[col] = le.fit_transform(df[col])

# Split features & target
X = df.drop("Churn", axis=1)
y = df["Churn"]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale numerical features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

print("Logistic Regression Results:")
print(classification_report(y_test, y_pred_lr))
print("ROC-AUC:", roc_auc_score(y_test, y_pred_lr))

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

print("Random Forest Results:")
print(classification_report(y_test, y_pred_rf))
print("ROC-AUC:", roc_auc_score(y_test, y_pred_rf))

sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix - Random Forest")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

importances = rf.feature_importances_
features = X.columns

feat_importance = pd.DataFrame({"Feature": features, "Importance": importances})
feat_importance = feat_importance.sort_values(by="Importance", ascending=False)

plt.figure(figsize=(10,6))
sns.barplot(x="Importance", y="Feature", data=feat_importance.head(10))
plt.title("Top 10 Features Driving Churn")
plt.show()

"""## Conclusion
- The Random Forest model performed better than Logistic Regression.  
- **Key drivers of churn**: Contract type, Tenure, Monthly Charges, Payment Method.  
- **Business Insight**: Customers with short contracts and higher monthly charges are more likely to churn.  

ðŸ‘‰ This model can help businesses identify at-risk customers and take preventive actions like offering discounts or loyalty rewards.

"""

